{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: Visual Pollution Detection  \n",
        "Model: YOLOv5s  \n",
        "Author: Shivendra Agarwal  \n",
        "Connect with me:  \n",
        "LinkedIn: https://www.linkedin.com/in/shivendra-agarwal-93027a141/    \n",
        "  \n",
        "Steps:\n",
        "1. Download Dataset\n",
        "2. Convert .csv annotation into Yolov5 readable format\n",
        "3. Divide into train, valid, test\n",
        "3. Create a base model with the Test and Valid images for benchmarking, with optimal data, epoch, batch, and image size and save the wieghts\n",
        "4. Image augmentation using different parameters on train and valid  \n",
        "  a. Brightness  \n",
        "  b. Saturation  \n",
        "  c. Grayscale  \n",
        "  d. Nosie  \n",
        "  e. Blur  \n",
        "  f. Hue(RBG)  \n",
        "5. Re-train using the older weights with the new set of data (original + augmented) and save.\n",
        "6. Run detect.py to test the model "
      ],
      "metadata": {
        "id": "8Qmii8wpaCzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "R8CFMOGYZt_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3wukULpZlaT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from imgaug import augmenters as iaa\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import shutil\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Download Dataset and view train.csv"
      ],
      "metadata": {
        "id": "03ef41pDdJGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_antt = pd.read_csv(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/train.csv\")\n",
        "df_antt.head()"
      ],
      "metadata": {
        "id": "6YLEpc6mZzUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Convert .csv annotation into Yolov5 readable format"
      ],
      "metadata": {
        "id": "fVEqBcPseVks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df_antt.iterrows():\n",
        "  img = row[\"image_path\"]\n",
        "  ls_bbox = df_antt.loc[df_antt.image_path == img].drop(\"image_path\",axis = 1).apply(lambda x: x.to_dict(),axis = 1).to_list()\n",
        "  print(ls_bbox)\n",
        "  img_dim = Image.open(os.path.join(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images\",img)).size\n",
        "  ls_out = []\n",
        "  print(img_dim)\n",
        "  for bbox in ls_bbox:\n",
        "    bbox_x = (bbox[\"xmin\"]*2 + bbox[\"xmax\"]*2) / (2 * img_dim[0])\n",
        "    bbox_y = (bbox[\"ymin\"]*2 + bbox[\"ymax\"]*2) / (2 * img_dim[1])\n",
        "    bbox_w = (bbox[\"xmax\"]*2 - bbox[\"xmin\"]*2) / img_dim[0]\n",
        "    bbox_h = (bbox[\"ymax\"]*2 - bbox[\"ymin\"]*2) / img_dim[1]\n",
        "    class_id = bbox[\"class\"]\n",
        "    ls_out.append(\"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(class_id,bbox_x,bbox_y,bbox_w,bbox_h))\n",
        "\n",
        "  fl_nm = os.path.join(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images\",img.replace(\"jpg\",\"txt\"))\n",
        "  print(\"\\n\".join(ls_out),file = open(fl_nm,\"w\"))"
      ],
      "metadata": {
        "id": "WOJPiaB7Z2n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Divide into train, valid, test\n",
        "\n",
        "\"\"\"   \n",
        "Train size  = 7874 images  \n",
        "test size = 2092 images  \n",
        "total =  9966  \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HAa2Pfjdepwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/test.csv\")\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "UzjNgbXmer_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seperate TEST\n",
        "for index, row in df_test.iterrows():\n",
        "  source = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/{row['image_path']}\"\n",
        "  destination = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/test/{row['image_path']}\"\n",
        "  print(source)\n",
        "  os.rename(source, destination)"
      ],
      "metadata": {
        "id": "BkrRY8EKev1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/test/\")\n",
        "print(\"test\", len(test_img))\n",
        "train_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/train/\")\n",
        "print(\"train\", len(train_img))\n",
        "valid_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/valid/images\")\n",
        "print(\"valid\", len(valid_img))"
      ],
      "metadata": {
        "id": "a4PmXecKfxB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# divide VALID and TRAIN\n",
        "split_list = []\n",
        "for iobject in train_img:\n",
        "  split_list.append(iobject[:-4])\n",
        "split_list = list(set(split_list))\n",
        "print(len(split_list))"
      ],
      "metadata": {
        "id": "5rmbgZUSfSgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"   \n",
        "train size = 6443  \n",
        "valid size = 1570 (approx) if we split in train into 80-20  \n",
        "test size = 2092  \n",
        "total = 9966  \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "h80iG4j8gjXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_img_list = []\n",
        "for i in range(0, 1570):\n",
        "  valid_img_list.append(random.choice(split_list))\n",
        "print(len(valid_img_list))\n",
        "print(valid_img_list)"
      ],
      "metadata": {
        "id": "Pzijkfaigiww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_img_list = list(set(valid_img_list))\n",
        "print(len(valid_img_list))"
      ],
      "metadata": {
        "id": "CWAKNdZYhOm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iobject in valid_img_list:\n",
        "  source = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/train/{iobject}.txt\"\n",
        "  destination = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/valid/{iobject}.txt\"\n",
        "  print(source)\n",
        "  shutil.move(source, destination)\n",
        "  source = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/train/{iobject}.jpg\"\n",
        "  destination = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/valid/{iobject}.jpg\"\n",
        "  print(source)\n",
        "  shutil.move(source, destination)"
      ],
      "metadata": {
        "id": "SlvUBH_NkZBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Make directory in YOLOv5 readable format"
      ],
      "metadata": {
        "id": "qTK3DVnClOMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/test/\")\n",
        "print(\"test\", len(test_img))\n",
        "train_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/train/\")\n",
        "print(\"train\", len(train_img))\n",
        "valid_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/valid/\")\n",
        "print(\"valid\", len(valid_img))"
      ],
      "metadata": {
        "id": "iNdzDzTkk0ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make two directories (images and labels) in /train/ and /valid/ and move .jpg files in images/ and .txt files in labels/"
      ],
      "metadata": {
        "id": "6bo-WQ1amEks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for VALID\n",
        "for iobject in os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/valid\"):\n",
        "  # for images\n",
        "  if iobject.endswith('.jpg'):\n",
        "    source = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/valid/{iobject}\"\n",
        "    destination = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/valid/images/{iobject}\"\n",
        "    print(source)\n",
        "    shutil.move(source, destination)\n",
        "  # for labels\n",
        "  elif iobject.endswith('.txt'):\n",
        "    source = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/valid/{iobject}\"\n",
        "    destination = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/valid/labels/{iobject}\"\n",
        "    print(source)\n",
        "    shutil.move(source, destination)"
      ],
      "metadata": {
        "id": "lOyzKiHKmTKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for TRAIN\n",
        "for iobject in os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/train\"):\n",
        "  # for images\n",
        "  if iobject.endswith('.jpg'):\n",
        "    source = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/train/{iobject}\"\n",
        "    destination = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/train/images/{iobject}\"\n",
        "    print(source)\n",
        "    shutil.move(source, destination)\n",
        "  # for labels\n",
        "  elif iobject.endswith('.txt'):\n",
        "    source = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/train/{iobject}\"\n",
        "    destination = f\"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/train/labels/{iobject}\"\n",
        "    print(source)\n",
        "    shutil.move(source, destination)"
      ],
      "metadata": {
        "id": "f7lXl1H_mvYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Make sure that your directory looks like this\n",
        "\n",
        "Dataset/images/    \n",
        "  1. train/  \n",
        "        images/ -> all training images  \n",
        "        labels/ -> all training labels  \n",
        "\n",
        "  2. valid/  \n",
        "        images/ -> all validation images  \n",
        "        labels/ -> all validation labels  \n",
        "\n",
        "  3. test/  \n",
        "        images/ -> all test images  \n",
        "        \n",
        "  4. data.yaml -> as format given below"
      ],
      "metadata": {
        "id": "NN1rMPQZm7x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### data.yaml\n",
        "train: ../../drive/MyDrive/SA-Garbage Detection/dataset/images/train/images  \n",
        "val: ../../drive/MyDrive/SA-Garbage Detection/dataset/images/valid/images  \n",
        "test: ../../drive/MyDrive/SA-Garbage Detection/dataset/images/test/images  \n",
        "\n",
        "nc: 11  \n",
        "names: ['GRAFFITI', 'FADED_SIGNAGE', 'POTHOLES', 'GARBAGE', 'CONSTRUCTION_ROAD', 'BROKEN_SIGNAGE', 'BAD_STREETLIGHT', 'BAD_BILLBOARD', 'SAND_ON_ROAD', 'CLUTTER_SIDEWALK', 'UNKEPT_FACADE']"
      ],
      "metadata": {
        "id": "avN2p8IOoGMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a base model with the Test and Valid images for benchmarking, with optimal data, epoch, batch, and image size and save the wieghts"
      ],
      "metadata": {
        "id": "BNsjQWlZp2ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAIyUTgNoz3w",
        "outputId": "ada7b540-1501-4f94-f904-f2e6029cb0b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-71-gc442a2e Python-3.8.10 torch-1.13.1+cu116 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 23.6/225.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy data.yaml to yolov5 folder\n",
        "!cp \"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/data.yaml\" /content/yolov5"
      ],
      "metadata": {
        "id": "7m6h7uk8pTv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train YOLOv5s on custom dataset for 30 epochs\n",
        "!python train.py --img 640 --batch 32 --epochs 30 --data data.yaml --weights yolov5s.pt --cache"
      ],
      "metadata": {
        "id": "WTG6FX1qpaYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save weights to drive\n",
        "!cp -r \"/content/yolov5/runs/train/exp\" \"/content/drive/MyDrive/SA-Garbage Detection\""
      ],
      "metadata": {
        "id": "aS9IcObgqH_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image augmentation (on 50% of dataset) using different parameters on train and valid"
      ],
      "metadata": {
        "id": "dpqfQgflqYmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/test/images\")\n",
        "print(\"test\", len(test_img))\n",
        "train_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/train/images\")\n",
        "print(\"train\", len(train_img))\n",
        "valid_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/valid/images\")\n",
        "print(\"valid\", len(valid_img))\n",
        "\n",
        "\n",
        "train_label = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/train/labels\")\n",
        "print(\"\\ntrain\", len(train_img))\n",
        "valid_label = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/valid/labels\")\n",
        "print(\"valid\", len(valid_img))"
      ],
      "metadata": {
        "id": "57aBuvIYqa1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def copy(source, destination):\n",
        "    try:\n",
        "      shutil.copy(source, destination)\n",
        "      print(\"File copied successfully.\")\n",
        "      return True\n",
        "# If source and destination are same\n",
        "    except:\n",
        "      return False"
      ],
      "metadata": {
        "id": "nJN4VK-Jq1mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory containing the images\n",
        "image_dir = \"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/train/images\"\n",
        "label_dir = \"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/train/labels\"\n",
        "\n",
        "aug_image_dir = \"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/train/images\"\n",
        "aug_label_dir = \"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/train/labels\"\n",
        "# Define augmentation sequences\n",
        "seq = iaa.Sequential([\n",
        "    iaa.MultiplyBrightness((0.5, 1.5)) #### CHANGE THIS LINE AS PER AUGMENTATION\n",
        "    ])\n",
        "count = 0\n",
        "total_img = len(os.listdir(image_dir))\n",
        "# Iterate over all image files in the directory\n",
        "for filename in os.listdir(image_dir):\n",
        "  if filename.endswith('.jpg'):\n",
        "      count = count + 1\n",
        "      print(f\"{count}/{total_img} - {filename}\")\n",
        "      # Open image file\n",
        "      image = Image.open(os.path.join(image_dir, filename))\n",
        "      # Convert image to numpy array\n",
        "      image = np.array(image)\n",
        "      # Determine if image should be augmented (50% chance)\n",
        "      if np.random.rand() < 0.5:\n",
        "          # Apply augmentations\n",
        "          image_aug = seq.augment_image(image)\n",
        "\n",
        "          # Save augmented image\n",
        "          Image.fromarray(image_aug).save(os.path.join(aug_image_dir, filename[:-4] + \"_aug\" + filename[-4:]))\n",
        "\n",
        "          #copy labels\n",
        "          aug_filename = f\"{filename[:-4]}_aug.txt\"\n",
        "\n",
        "          source = f\"{label_dir}/{filename[:-4]}.txt\"\n",
        "          destination = f\"{aug_label_dir}/{aug_filename}\"\n",
        "\n",
        "          copy(source, destination)\n",
        "\n",
        "          # Save original image\n",
        "          Image.fromarray(image).save(os.path.join(aug_image_dir, filename))\n",
        "          #copy labels\n",
        "          aug_filename = f\"{filename[:-4]}.txt\"\n",
        "          source = f\"{label_dir}/{filename[:-4]}.txt\"\n",
        "          destination = f\"{aug_label_dir}/{filename[:-4]}.txt\"\n",
        "          copy(source, destination)\n",
        "\n",
        "\n",
        "      else:\n",
        "          # Save original image\n",
        "          Image.fromarray(image).save(os.path.join(aug_image_dir, filename))\n",
        "          #copy labels\n",
        "          aug_filename = f\"{filename[:-4]}.txt\"\n",
        "          source = f\"{label_dir}/{filename[:-4]}.txt\"\n",
        "          destination = f\"{aug_label_dir}/{filename[:-4]}.txt\"\n",
        "          copy(source, destination)"
      ],
      "metadata": {
        "id": "sG7Ij8_BqmwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the directory path in the above cell and run for valid images"
      ],
      "metadata": {
        "id": "GceKVrXEr69j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Brightness\n",
        "\n",
        "Define augmentation sequences\n",
        "```\n",
        "seq = iaa.Sequential([\n",
        "    iaa.MultiplyBrightness((0.5, 1.5))\n",
        "    ])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "tVMNkNXfqw7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saturation\n",
        "\n",
        "Define augmentation sequences\n",
        "```\n",
        "seq = iaa.Sequential([\n",
        "    iaa.MultiplySaturation(0.5) # Saturation\n",
        "    ])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "wks3LNPUrvuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Grayscale\n",
        "\n",
        "Define augmentation sequences\n",
        "```\n",
        "seq = iaa.Sequential([\n",
        "    iaa.Grayscale(alpha=1.0)\n",
        "    ])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "hflkhT7LsP_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Noise\n",
        "\n",
        "Define augmentation sequences\n",
        "```\n",
        "seq = iaa.Sequential([\n",
        "    iaa.AdditiveGaussianNoise(scale=(0, 0.05*255)), # Add gaussian noise with a scale from 0 to 0.05*255 \n",
        "    ])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "koRdLbDKsa2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Blur\n",
        "\n",
        "Define augmentation sequences\n",
        "```\n",
        "seq = iaa.Sequential([\n",
        "    iaa.GaussianBlur(sigma=(0, 2.0)), # add gaussian blur with a random sigma from 0 to 3\n",
        "    ])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OQl1pex5sh5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hue\n",
        "\n",
        "Define augmentation sequences  \n",
        "Red\n",
        "```\n",
        "seq = iaa.WithColorspace(to_colorspace=\"HSV\", from_colorspace=\"RGB\", children=iaa.WithChannels(0, iaa.Add((0, 50))))\n",
        "    ])\n",
        "```\n",
        "Green\n",
        "```\n",
        "seq = iaa.WithColorspace(to_colorspace=\"HSV\", from_colorspace=\"RGB\", children=iaa.WithChannels(1, iaa.Add((0, 50))))\n",
        "    ])\n",
        "```\n",
        "Blue\n",
        "```\n",
        "seq = iaa.WithColorspace(to_colorspace=\"HSV\", from_colorspace=\"RGB\", children=iaa.WithChannels(2, iaa.Add((0, 50))))\n",
        "    ])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SCExlhq7soHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please change line no. 9 in the above code cell to use any of the augmentation technique mentioned above"
      ],
      "metadata": {
        "id": "veTEYKm2s_qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### np.random.rand() - to apply only on selected range of images\n",
        "Augment Technique | train | valid | np.random.rand() <\n",
        "--- | --- | --- | ---\n",
        "**Raw** | 6443 | 1431 | 0.5\n",
        "**Brightness** | 9660 | 2169 |0.5\n",
        "**Saturation** | 12897 | 2869 | 0.5\n",
        "**Grayscale** | 16163 | 3618 | 0.5\n",
        "**Noise** | 19440 | 4340 | 0.5\n",
        "**Blur** | 22690 | 5060 | 0.5\n",
        "**Hue-R** | 23277 | 5206 | 0.1\n",
        "**Hue-G** | 23798 | 5371 | 0.1\n",
        "**Hue-B** | 24270 | 5499 | 0.1\n",
        "\n"
      ],
      "metadata": {
        "id": "lpNTZjyptnV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/test/images\")\n",
        "print(\"test\", len(test_img))\n",
        "train_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/train/images\")\n",
        "print(\"train\", len(train_img))\n",
        "valid_img = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/valid/images\")\n",
        "print(\"valid\", len(valid_img))\n",
        "\n",
        "\n",
        "train_label = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/train/labels\")\n",
        "print(\"\\ntrain\", len(train_img))\n",
        "valid_label = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/aug_images/valid/labels\")\n",
        "print(\"valid\", len(valid_img))"
      ],
      "metadata": {
        "id": "yxOUAPWivh9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-train using the older weights with the new set of data (original + augmented) and save."
      ],
      "metadata": {
        "id": "oBGwoCxEwnKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### change dataset path in data.yaml\n",
        "train: ../../drive/MyDrive/SA-Garbage Detection/dataset/aug_images/train/images  \n",
        "val: ../../drive/MyDrive/SA-Garbage Detection/dataset/aug_images/valid/images  \n",
        "test: ../../drive/MyDrive/SA-Garbage Detection/dataset/aug_images/test/images  \n",
        "\n",
        "nc: 11  \n",
        "names: ['GRAFFITI', 'FADED_SIGNAGE', 'POTHOLES', 'GARBAGE', 'CONSTRUCTION_ROAD', 'BROKEN_SIGNAGE', 'BAD_STREETLIGHT', 'BAD_BILLBOARD', 'SAND_ON_ROAD', 'CLUTTER_SIDEWALK', 'UNKEPT_FACADE']"
      ],
      "metadata": {
        "id": "7Q2lIDa_wYQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy data.yaml to yolov5 folder\n",
        "!cp \"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/data.yaml\" /content/yolov5"
      ],
      "metadata": {
        "id": "V-4Ie29Rwhw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy previous weight to yolov5 folder\n",
        "!cp \"/content/drive/MyDrive/SA-Garbage Detection/exp5_exp4_batch32_epoch30_img640_aug_yolov5s/weights/best.pt\" /content/yolov5"
      ],
      "metadata": {
        "id": "T1LU6dZZx0dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train YOLOv5s on custom dataset for 130 epochs\n",
        "!python train.py --img 640 --batch 32 --epochs 130 --data data.yaml --weights best.pt --cache"
      ],
      "metadata": {
        "id": "Dy5hux-_x8Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/yolov5/runs/train/exp2\" \"/content/drive/MyDrive/SA-Garbage Detection\""
      ],
      "metadata": {
        "id": "dbivBCIA3PeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate the model"
      ],
      "metadata": {
        "id": "Rk4KT9pm3_SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate YOLOv5s on custom dataset\n",
        "!python val.py --weights \"/content/drive/MyDrive/SA-Garbage Detection/exp5_exp4_batch32_epoch30_img640_aug_yolov5s/weights/best.pt\" --data data.yaml --img 640 --half"
      ],
      "metadata": {
        "id": "7AvD-XwR4Bxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detect to test the model"
      ],
      "metadata": {
        "id": "ekWJDHCU37TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --save-txt --weights \"/content/drive/MyDrive/SA-Garbage Detection/exp5_exp4_batch32_epoch30_img640_aug_yolov5s/weights/best.pt\" --img 640 --conf 0.5 --source \"/content/drive/MyDrive/SA-Garbage Detection/dataset/images/test/images\"\n",
        "# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
      ],
      "metadata": {
        "id": "BgX_iPFx4L4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert yolov5 labels to xmin, ymin, xmax, ymax"
      ],
      "metadata": {
        "id": "pA-i44J44SSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_img_labels(img_name, str_v):\n",
        "  cvmat = cv2.imread(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/output/\" + img_name[:-4] + \".jpg\")\n",
        "\n",
        "  #get height, width\n",
        "  h,w,_ = cvmat.shape\n",
        "\n",
        "  #extract x1, y1 <- center, width, height\n",
        "  class_id = int( float(str_v.split(' ')[0]))\n",
        "  x1 = int( float(str_v.split(' ')[1]) * w )\n",
        "\n",
        "  y1 = int( float(str_v.split(' ')[2]) * h )\n",
        "\n",
        "  xw = int( float(str_v.split(' ')[3]) * w /2)\n",
        "\n",
        "  yw = int( float(str_v.split(' ')[4]) * h /2)\n",
        "\n",
        "  #make x1,y1, x2,y2\n",
        "\n",
        "  start_point = (x1 - xw, y1 - yw )\n",
        "\n",
        "  end_point   = (x1 + xw, y1 + yw )\n",
        "  # Define the bounding box coordinates\n",
        "  xmin = start_point[0]\n",
        "  ymin = start_point[1]\n",
        "  xmax = end_point[0]\n",
        "  ymax = end_point[1]\n",
        "  return class_id, xmin, ymin, xmax, ymax"
      ],
      "metadata": {
        "id": "Q7nXvA3P4Yhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_test_list = []\n",
        "class_name = ['GRAFFITI', 'FADED_SIGNAGE', 'POTHOLES', 'GARBAGE', 'CONSTRUCTION_ROAD', 'BROKEN_SIGNAGE', 'BAD_STREETLIGHT', 'BAD_BILLBOARD', 'SAND_ON_ROAD', 'CLUTTER_SIDEWALK', 'UNKEPT_FACADE']\n",
        "labels_list = os.listdir(\"/content/drive/MyDrive/SA-Garbage Detection/dataset/output/labels\")\n",
        "for iobject in labels_list:\n",
        "  with open(os.path.join('/content/drive/MyDrive/SA-Garbage Detection/dataset/output/labels', iobject), 'r') as f:\n",
        "    labels = f.read()\n",
        "    print(labels.split('\\n'))\n",
        "    for jobject in labels.split('\\n'):\n",
        "      if len(jobject) > 0:\n",
        "        class_id, xmin, ymin, xmax, ymax = get_img_labels(iobject, jobject)\n",
        "        print(class_id, xmin, ymin, xmax, ymax)\n",
        "        final_test_list.append({\"class\": class_id, \"image_path\": iobject[:-4]+\".jpg\", \"name\": class_name[class_id], \"xmax\": xmax, \"xmin\": xmin, \"ymax\": ymax, \"ymin\": ymin})        "
      ],
      "metadata": {
        "id": "1ftrQKQn4gp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_to_df = pd.DataFrame.from_dict(final_test_list)"
      ],
      "metadata": {
        "id": "ltzqAM2U4krX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_to_df.to_csv('/content/drive/MyDrive/SA-Garbage Detection/dataset/test.csv', index=False)"
      ],
      "metadata": {
        "id": "mcqz_6-34l63"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}